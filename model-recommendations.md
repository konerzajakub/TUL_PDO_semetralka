# üöÄ Struƒçn√Ω n√°vod pro v√Ωbƒõr jazykov√©ho modelu

## üß† D≈Øle≈æit√© term√≠ny

* **GGUF:** Popul√°rn√≠ form√°t soubor≈Ø pro LLM, optimalizovan√Ω pro bƒõh na CPU i GPU.
* **Kvantizace:** Technika zmen≈°en√≠ modelu useknut√≠m p≈ôenosti vah. V√Ωsledkem je zmen≈°en√≠ velikosti za cenu p≈ôesnosti.
    * **Typy:** Od `Q2_K` (nejmen≈°√≠, nejrychlej≈°√≠, nejni≈æ≈°√≠ kvalita) po `Q8_0` (vƒõt≈°√≠, pomalej≈°√≠, vy≈°≈°√≠ kvalita)
    * **Nejvy≈°≈°√≠ kvalita (`F16`, `F32` - nekvantizovan√©):** Tyto modely nab√≠zej√≠ p≈Øvodn√≠ p≈ôesnost po tr√©nov√°n√≠, ale jsou velmi n√°roƒçn√©.
        * **Po≈æadovan√Ω HW:** Nejlep≈°√≠ GPU na trhu s velkou kapacitou VRAM (nap≈ô. hern√≠ NVIDIA RTX 4090 s 24GB VRAM, profesion√°ln√≠ karty jako NVIDIA A100/H100 s 80GB+ VRAM).
        * **Pro bƒõ≈æn√© u≈æivatele:** ≈òe≈°en√≠m m≈Ø≈æe b√Ωt pron√°jem cloudov√Ωch server≈Ø (AWS, GCP, Azure)

---

## üõ†Ô∏è Co zv√°≈æit p≈ôed v√Ωbƒõrem modelu?

1. **V√Ωpoƒçetn√≠ zdroje:** Modely mohou bƒõ≈æet buƒè na procesoru (CPU) a operaƒçn√≠ pamƒõti (RAM), nebo na grafick√© kartƒõ (GPU) a jej√≠ vlastn√≠ pamƒõti (vRAM).

   - **CPU + RAM:** Tato kombinace je vhodn√°, pokud m√°te siln√Ω procesor a dostatek operaƒçn√≠ pamƒõti, a nem√°te v√Ωkonnou grafickou kartu. Model bƒõ≈æ√≠ p≈ô√≠mo na procesoru a m≈Ø≈æe to b√Ωt pou≈æiteln√©.
   - **GPU + vRAM:** Pokud m√°te v√Ωkonnou dedikovanou grafickou kartu s dostateƒçnou vRAM, tohle je ta nejlep≈°√≠ volba. 

2. **RAM:** M√°te k dispozici dostatek operaƒçn√≠ pamƒõti pro ulo≈æen√≠ vah modelu do pamƒõti RAM?
3. **VRAM (GPU):** M√°te k dispozici dostatek operaƒçn√≠ pamƒõti pro ulo≈æen√≠ vah modelu do pamƒõti vRAM?

---

## üìä Doporuƒçen√≠ model≈Ø (GGUF) dle HW (kvƒõten 2025)

| HW Konfigurace                     | Doporuƒçen√° kvantizace         | Velikost* | P≈ô√≠klady model≈Ø (GGUF)                    |
|------------------------------------|-------------------------------|-----------|-------------------------------------------|
| ‚â§ 8GB RAM / integrovan√° GPU        | `Q3_K_S`, `Q4_0`              | ~2-4 GB   | `Phi-3-mini-4k-instruct.Q4_0.gguf`        |
| 8-16GB RAM / 8GB VRAM (GPU)      | `Q4_K_M`, `Q5_0`              | ~4-7 GB   | `Llama-3-8B-Instruct.Q4_K_M.gguf`         |
| 16-32GB RAM / 12GB VRAM (GPU)    | `Q5_K_M`, `Q6_K`              | ~6-10 GB  | `Llama-3-8B-Instruct.Q5_K_M.gguf`         |
| ‚â• 32GB RAM / 16GB+ VRAM (GPU)      | `Q6_K`, `Q8_0`                | 10GB+     | `Llama-3-8B-Instruct.Q8_0.gguf`, nebo vƒõt≈°√≠ modely jako `Llama-3-70B` s agresivn√≠ kvantizac√≠ (nap≈ô. `Q4_K_M`) |


| Model                                  | Velikost (Parametry) | Chatbot Arena Elo (P≈ôibli≈ænƒõ) | Odhadovan√° VRAM (Q4_K_M) | Pozn√°mka k hostov√°n√≠ / Kvalitƒõ                                  |
|----------------------------------------|----------------------|-------------------------------|--------------------------|-----------------------------------------------------------------|
| **Mal√© a efektivn√≠ modely**            |                      |                               |                          |                                                                 |
| `Phi-3-mini-4k-instruct`               | 3.8 Miliard          | ~1120-1150                     | ~2.4 GB                  | Velmi schopn√Ω na svou velikost, skvƒõl√Ω pro omezen√© zdroje. |
| `Mistral-7B-Instruct-v0.2`             | 7 Miliard            | ~1100-1130                     | ~4.4 GB                  | St√°le velmi popul√°rn√≠, dobr√Ω kompromis.           |
| `Llama-3-8B-Instruct`                  | 8 Miliard            | ~1150-1180                     | ~4.9 GB                  | V√Ωborn√Ω v√Ωkon ve sv√© t≈ô√≠dƒõ, top volba pro mnoho u≈æivatel≈Ø.  |
| **St≈ôednƒõ velk√© modely**               |                      |                               |                          |                                                                 |
| `Qwen1.5-14B-Chat`                     | 14 Miliard           | ~1100-1140                     | ~8.0 GB                  | Siln√Ω model od Alibaba, dobr√© v√≠cejazyƒçn√© schopnosti. |
| `Phi-3-medium-4k-instruct`             | 14 Miliard           | ~1120-1150                     | ~7.0 GB (odhad)          | V√Ωkonnƒõj≈°√≠ verze Phi-3. [1]                                     |
| **Vƒõt≈°√≠ a v√Ωkonnƒõj≈°√≠ modely**         |                      |                               |                          |                                                                 |
| `Mixtral-8x7B-Instruct-v0.1` (MoE)   | 46.7 Miliard (efektivnƒõ ~13B aktivn√≠ch) | ~1110-1140 | ~25-30 GB                | Skvƒõl√° kvalita, ale n√°roƒçnƒõj≈°√≠ na VRAM.        |
| `Llama-3-70B-Instruct`                 | 70 Miliard           | ~1200-1230                     | ~40-43 GB                | ≈†piƒçkov√Ω open-source model, vy≈æaduje v√Ωkonn√Ω HW.  |
| `Qwen1.5-72B-Chat`                     | 72 Miliard           | ~1180-1210                     | ~40-45 GB (odhad)        | Velmi v√Ωkonn√Ω, konkurent Llama 3 70B.             |
| `Command R+` (Cohere)                  | 104 Miliard          | ~1190-1220                     | ~60-63 GB                | Extr√©mnƒõ schopn√Ω, ale velmi n√°roƒçn√Ω na zdroje.   |
