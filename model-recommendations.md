# ğŸš€ PÅ™Ã­ruÄka k vÃ½bÄ›ru jazykovÃ©ho modelu

## ğŸ§  DÅ¯leÅ¾itÃ© termÃ­ny

* **GGUF:** PopulÃ¡rnÃ­ formÃ¡t souborÅ¯ pro LLM, optimalizovanÃ½ pro bÄ›h na CPU i GPU.
* **Kvantizace:** Technika zmenÅ¡enÃ­ modelu useknutÃ­m pÅ™enosti vah. VÃ½sledkem je zmenÅ¡enÃ­ velikosti za cenu pÅ™esnosti.
    * **Typy:** Od `Q2_K` (nejmenÅ¡Ã­, nejrychlejÅ¡Ã­, nejniÅ¾Å¡Ã­ kvalita) po `Q8_0` (vÄ›tÅ¡Ã­, pomalejÅ¡Ã­, vyÅ¡Å¡Ã­ kvalita)
    * **NejvyÅ¡Å¡Ã­ kvalita (`F16`, `F32` - nekvantizovanÃ©):** Tyto modely nabÃ­zejÃ­ pÅ¯vodnÃ­ pÅ™esnost po trÃ©novÃ¡nÃ­, ale jsou velmi nÃ¡roÄnÃ©.
        * **PoÅ¾adovanÃ½ HW:** NejlepÅ¡Ã­ GPU na trhu s velkou kapacitou VRAM (napÅ™. hernÃ­ NVIDIA RTX 4090 s 24GB VRAM, profesionÃ¡lnÃ­ karty jako NVIDIA A100/H100 s 80GB+ VRAM).
        * **Pro bÄ›Å¾nÃ© uÅ¾ivatele:** Å˜eÅ¡enÃ­m mÅ¯Å¾e bÃ½t pronÃ¡jem cloudovÃ½ch serverÅ¯ (AWS, GCP, Azure)

---

## ğŸ› ï¸ Co zvÃ¡Å¾it pÅ™ed vÃ½bÄ›rem modelu?

1. **VÃ½poÄetnÃ­ zdroje:** Modely mohou bÄ›Å¾et buÄ na procesoru (CPU) a operaÄnÃ­ pamÄ›ti (RAM), nebo na grafickÃ© kartÄ› (GPU) a jejÃ­ vlastnÃ­ pamÄ›ti (vRAM).

   - **CPU + RAM:** Tato kombinace je vhodnÃ¡, pokud mÃ¡te silnÃ½ procesor a dostatek operaÄnÃ­ pamÄ›ti, a nemÃ¡te vÃ½konnou grafickou kartu. Model bÄ›Å¾Ã­ pÅ™Ã­mo na procesoru a mÅ¯Å¾e to bÃ½t pouÅ¾itelnÃ©.
   - **GPU + vRAM:** Pokud mÃ¡te vÃ½konnou dedikovanou grafickou kartu s dostateÄnou vRAM, tohle je ta nejlepÅ¡Ã­ volba. 

2. **RAM:** MÃ¡te k dispozici dostatek operaÄnÃ­ pamÄ›ti pro uloÅ¾enÃ­ vah modelu do pamÄ›ti RAM?
3. **VRAM (GPU):** MÃ¡te k dispozici dostatek operaÄnÃ­ pamÄ›ti pro uloÅ¾enÃ­ vah modelu do pamÄ›ti vRAM?

---

## ğŸ“Š DoporuÄenÃ­ modelÅ¯ (GGUF) dle HW

| Model                                  | Velikost (Parametry) | OdhadovanÃ© nÃ¡roky na pamÄ›Å¥ (Q4_K_M) |
|----------------------------------------|----------------------|--------------------------|
| **MalÃ© a efektivnÃ­ modely**            |                      |                          |
| `Phi-3-mini-4k-instruct`               | 3.8 Miliard          | ~2.4 GB                  |
| `Ministral-8B-2410`                    | 8 Miliard            | ~4.9 GB                  |
| `Meta-Llama-3.1-8B-Instruct`           | 8 Miliard            | ~4.9 GB                  |
| **StÅ™ednÄ› velkÃ© modely**               |                      |                          |
| `Qwen1.5-14B-Chat`                     | 14 Miliard           | ~8.0 GB                  |
| `Phi-3-medium-4k-instruct`             | 14 Miliard           | ~7.0 GB           |
| **VÄ›tÅ¡Ã­ a vÃ½konnÄ›jÅ¡Ã­ modely**         |                      |                          |
| `Mixtral-8x7B-Instruct-v0.1` (MoE)   | 46.7 Miliard  | ~25-30 GB                |
| `Llama-3-70B-Instruct`                 | 70 Miliard           | ~40-43 GB                |
| `Qwen2-72B-Instruct`                   | 72 Miliard           | ~40-45 GB     |
| `Command R+` (Cohere)                  | 104 Miliard          | ~60-63 GB                |     
