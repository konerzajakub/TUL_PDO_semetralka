# ğŸš€ StruÄnÃ½ nÃ¡vod pro vÃ½bÄ›r jazykovÃ©ho modelu

## ğŸ§  DÅ¯leÅ¾itÃ© termÃ­ny

* **GGUF:** PopulÃ¡rnÃ­ formÃ¡t souborÅ¯ pro LLM, optimalizovanÃ½ pro bÄ›h na CPU i GPU.
* **Kvantizace:** Technika zmenÅ¡enÃ­ modelu. VÃ½sledkem je kompromis mezi velikostÃ­/rychlostÃ­ a pÅ™esnostÃ­.
    * **Typy:** Od `Q2_K` (nejmenÅ¡Ã­, nejrychlejÅ¡Ã­, nejniÅ¾Å¡Ã­ kvalita) po `Q8_0` (vÄ›tÅ¡Ã­, pomalejÅ¡Ã­, vyÅ¡Å¡Ã­ kvalita)
    * **NejvyÅ¡Å¡Ã­ kvalita (`F16`, `F32` - nekvantizovanÃ©/mÃ©nÄ› kvantizovanÃ©):** Tyto modely nabÃ­zejÃ­ maximÃ¡lnÃ­ pÅ™esnost, ale jsou extrÃ©mnÄ› nÃ¡roÄnÃ©.
        * **PoÅ¾adovanÃ½ HW:** Vysoce vÃ½konnÃ© GPU s velkou VRAM (napÅ™. NVIDIA RTX 4090 s 24GB VRAM, profesionÃ¡lnÃ­ karty jako NVIDIA A100/H100 s 80GB+ VRAM).
        * **Pro bÄ›Å¾nÃ© uÅ¾ivatele:** Å˜eÅ¡enÃ­m mÅ¯Å¾e bÃ½t **pronÃ¡jem cloudovÃ½ch serverÅ¯** (AWS, GCP, Azure)

---

## ğŸ› ï¸ Co zvÃ¡Å¾it pÅ™ed vÃ½bÄ›rem modelu?

1.  **RAM:** Kolik mÃ¡te k dispozici operaÄnÃ­ pamÄ›ti? (napÅ™. 8GB, 16GB, 32GB+)
2.  **VRAM (GPU):** MÃ¡te dedikovanou grafickou kartu a kolik mÃ¡ pamÄ›ti (vRAM)? (napÅ™. 4GB, 8GB, 16GB+)

---

## ğŸ“Š DoporuÄenÃ­ modelÅ¯ (GGUF) dle HW (kvÄ›ten 2025)

| HW Konfigurace                     | DoporuÄenÃ¡ kvantizace         | Velikost* | PÅ™Ã­klady modelÅ¯ (GGUF)                    |
|------------------------------------|-------------------------------|-----------|-------------------------------------------|
| â‰¤ 8GB RAM / integrovanÃ¡ GPU        | `Q3_K_S`, `Q4_0`              | ~2-4 GB   | `Phi-3-mini-4k-instruct.Q4_0.gguf`        |
| 8-16GB RAM / 8GB VRAM (GPU)      | `Q4_K_M`, `Q5_0`              | ~4-7 GB   | `Llama-3-8B-Instruct.Q4_K_M.gguf`         |
| 16-32GB RAM / 12GB VRAM (GPU)    | `Q5_K_M`, `Q6_K`              | ~6-10 GB  | `Llama-3-8B-Instruct.Q5_K_M.gguf`         |
| â‰¥ 32GB RAM / 16GB+ VRAM (GPU)      | `Q6_K`, `Q8_0` | 10GB+     | `Llama-3-8B-Instruct.Q8_0.gguf`, nebo vÄ›tÅ¡Ã­ modely jako `Llama-3-70B` s agresivnÃ­ kvantizacÃ­ (napÅ™. `Q4_K_M`) |

> **DÅ¯leÅ¾itÃ©:** VÄ›tÅ¡Ã­ zÃ¡kladnÃ­ modely (napÅ™. 70B) budou i po silnÃ© kvantizaci stÃ¡le vyÅ¾adovat vÃ½raznÄ› vÃ­ce zdrojÅ¯ neÅ¾ menÅ¡Ã­ modely (napÅ™. 8B) se stejnou kvantizacÃ­.
